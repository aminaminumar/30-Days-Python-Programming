{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Datetime Exercises\n",
    "1. Get the current day, month, year, hour, minute and timestamp from datetime module\n",
    "2. Format the current date using this format: \"%m/%d/%Y, %H:%M:%S\")\n",
    "3. Today is 5 December, 2019. Change this time string to time.\n",
    "4. Calculate the time difference between now and new year.\n",
    "5. Calculate the time difference between 1 January 1970 and now.\n",
    "6. Think, what can you use the datetime module for? Examples:\n",
    "7. Time series analysis\n",
    "8. To get a timestamp of any activities in an application\n",
    "9. Adding posts on a blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:38:03.238237\n",
      "10-1-2025 Time:15:38pm Timestamp: 1736519883.238237\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the current day, month, year, hour, minute and timestamp from datetime module\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(now)\n",
    "\n",
    "day = now.day\n",
    "month = now.month\n",
    "year = now.year\n",
    "hour = now.hour\n",
    "minute = now.minute\n",
    "seconds = now.second\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "print(f\"{day}-{month}-{year} Time:{hour}:{minute}pm Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todays Date:01/10/2025 Time: 15:38:03\n",
      "15:38:03\n",
      "01/10/2025\n"
     ]
    }
   ],
   "source": [
    "# 2. Format the current date using this format: \"%m/%d/%Y, %H:%M:%S\")\n",
    "Date= now.strftime(\"%m/%d/%Y\")\n",
    "Time = now.strftime(\"%H:%M:%S\")\n",
    "print(f\"Todays Date:{Date} Time: {Time}\")\n",
    "print(Time)\n",
    "print(Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 December, 2023\n",
      "2023-12-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# 3. Today is 5 December, 2019. Change this time string to time.\n",
    "today_date_string= \"5 December, 2023\"\n",
    "today_date_object = datetime.strptime(today_date_string, \"%d %B, %Y\")\n",
    "\n",
    "print(today_date_string)\n",
    "print(today_date_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Calculate the time difference between now and new year.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m Today \u001b[38;5;241m=\u001b[39m \u001b[43mdate\u001b[49m(year \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025\u001b[39m\u001b[38;5;124m'\u001b[39m, month \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJanuary\u001b[39m\u001b[38;5;124m'\u001b[39m,day \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m )\n\u001b[0;32m      4\u001b[0m new_year \u001b[38;5;241m=\u001b[39m date(year \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2026\u001b[39m\u001b[38;5;124m'\u001b[39m, month \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecember\u001b[39m\u001b[38;5;124m'\u001b[39m,day \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m)\n\u001b[0;32m      6\u001b[0m time_left_for_newyear \u001b[38;5;241m=\u001b[39m Today \u001b[38;5;241m-\u001b[39m new_year\n",
      "\u001b[1;31mNameError\u001b[0m: name 'date' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Calculate the time difference between now and new year.\n",
    "import datetime\n",
    "Today = date(year = '2025', month = 'January',day = 17 )\n",
    "new_year = date(year = '2026', month = 'December',day =31)\n",
    "\n",
    "time_left_for_newyear = Today - new_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 = 86 days, 22:56:50\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "t1 = timedelta(weeks=12, days=10, hours=4, seconds=20)\n",
    "t2 = timedelta(days=7, hours=5, minutes=3, seconds=30)\n",
    "t3 = t1 - t2\n",
    "print(\"t3 =\", t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Handling Exercise\n",
    "1. Write a function which count number of lines and number of words in a text. All the files are in the data the folder: a) Read obama_speech.txt file and count number of lines and words b) Read michelle_obama_speech.txt file and count number of lines and words c) Read donald_speech.txt file and count number of lines and words d) Read melina_trump_speech.txt file and count number of lines and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fellow citizens:I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a file obama speech\n",
    "\n",
    "with open(\"Obama_Speech.txt\", 'r') as file:\n",
    "    txt = file.readline()\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines:66\n",
      "number of words:2400\n"
     ]
    }
   ],
   "source": [
    "# Write a function which count number of lines and number of words in a text. All the files are in the data the folder:\n",
    "# a) Read obama_speech.txt file and count number of lines and words\n",
    "\n",
    "def num_of_lines_words(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        num_lines = len(lines)\n",
    "        num_words = sum(len(line.split()) for line in lines)\n",
    "    return num_lines, num_words\n",
    "\n",
    "\n",
    "# lets run our function\n",
    "file_path = \"obama_speech.txt\"\n",
    "num_lines, num_words = num_of_lines_words(file_path)\n",
    "print(f\"number of lines:{num_lines}\\nnumber of words:{num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='Michelle_obama_speech.txt' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a file michele speech\n",
    "\n",
    "open(\"Michelle_obama_speech.txt\", 'w') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in Obama speech is:83\n",
      "Number of words in Obama speech is:2204\n"
     ]
    }
   ],
   "source": [
    "# Read michelle_obama_speech.txt file and count number of lines and words\n",
    "file = 'Michelle_obama_speech.txt'\n",
    "nummbe_lines, number_words = num_of_lines_words(file)\n",
    "print(f\"Number of lines in Obama speech is:{nummbe_lines}\\nNumber of words in Obama speech is:{number_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='Donalt_Speech.txt' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a file donalt speech\n",
    "\n",
    "open(\"Donalt_Speech.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1259)\n"
     ]
    }
   ],
   "source": [
    "# count number of lines and words in donalt speech\n",
    "print(num_of_lines_words(\"Donalt_Speech.txt\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='Melina_trump_speech.txt' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the file melina_trump speech\n",
    "open(\"Melina_trump_speech.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 1375)\n",
      "(33, 1375)\n"
     ]
    }
   ],
   "source": [
    "# Read melina_trump_speech.txt file and count number of\n",
    "#  lines and words\n",
    "\n",
    "print(num_of_lines_words('Melina_trump_speech'))\n",
    "print(num_of_lines_words('Melina_trump_speech.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the countries_data.json data file in data directory, create a function that finds the ten most spoken languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', 91), ('French', 45)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# function to find the most common language\n",
    "def most_common_langauge(file_path, n):\n",
    "    with open(file_path, 'r', encoding = 'utf-8' ) as f:\n",
    "        countries_data = json.load(f)\n",
    "\n",
    "    langauge_counter = Counter()\n",
    "    for country in countries_data:\n",
    "        languages = country.get('languages', [])\n",
    "        langauge_counter.update(languages)\n",
    "\n",
    "    most_spoken_languages = langauge_counter.most_common(n)\n",
    "    return most_spoken_languages\n",
    "\n",
    "# usage:\n",
    "file_path = \"./countries_data.json\"\n",
    "print(most_common_langauge(file_path, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the countries_data.json data file in data directory, create a function that creates a list of the ten most populated countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}, {'country': 'Indonesia', 'population': 258705000}, {'country': 'Brazil', 'population': 206135893}, {'country': 'Pakistan', 'population': 194125062}, {'country': 'Nigeria', 'population': 186988000}, {'country': 'Bangladesh', 'population': 161006790}, {'country': 'Russian Federation', 'population': 146599183}, {'country': 'Japan', 'population': 126960000}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "def populated_countries(file_path, n):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        countries_data = json.load(f)\n",
    "    # sort the countries according to population\n",
    "    sorted_countries = sorted(countries_data, key = lambda x: x['population'], reverse = True )\n",
    "\n",
    "    # now print the countires using list comprehension\n",
    "    most_populated_countries = [{'country': country['name'], 'population': country['population'] }for country in sorted_countries[:n]]\n",
    "\n",
    "    return most_populated_countries\n",
    "\n",
    "# usage\n",
    "\n",
    "file_path = \"./countries_data.json\"\n",
    "print(populated_countries(file_path, 10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country:China, Population:1377422166\n",
      "Country:India, Population:1295210000\n",
      "Country:United States of America, Population:323947000\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def populated_countries(file_path, n):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        countries_data = json.load(f)\n",
    "    # sort the countries according to population\n",
    "    sorted_countries = sorted(countries_data, key = lambda x: x['population'], reverse = True )\n",
    "\n",
    "    # now print the countires using for loop\n",
    "    for i in range(n):\n",
    "        country = sorted_countries[i]\n",
    "        print(f\"Country:{country['name']}, Population:{country['population']}\")\n",
    "\n",
    "# usage\n",
    "\n",
    "file_path = \"./countries_data.json\"\n",
    "print(populated_countries(file_path, 3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract all incoming email addresses as a list from the email_exchange_big.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='email_exchanges.txt' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('email_exchanges.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression pattern for matching email addresses\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Example usage\n",
    "text = \"Please contact us at support@example.com for further information. You can also reach out to admin@domain.org.\"\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'david.horwitz@uct.ac.za', 'ray@media.berkeley.edu', '200801041610.m04GA5KP007209@nakamura.uits.iupui.edu', '200801040905.m0495rWB006420@nakamura.uits.iupui.edu', 'stephen.marquard@uct.ac.za', 'cwen@iupui.edu', 'zqian@umich.edu', '200801041611.m04GB1Lb007221@nakamura.uits.iupui.edu', '200801041608.m04G8d7w007184@nakamura.uits.iupui.edu', '200801041635.m04GZQGZ007313@nakamura.uits.iupui.edu', '200801041502.m04F21Jo007031@nakamura.uits.iupui.edu', '200801040932.m049W2i5006493@nakamura.uits.iupui.edu', 'rjlowe@iupui.edu', '200801041515.m04FFv42007050@nakamura.uits.iupui.edu', 'wagnermr@iupui.edu', 'gopal.ramasammycook@gmail.com', '200801041200.m04C0gfK006793@nakamura.uits.iupui.edu', '200801032127.m03LRUqH005177@nakamura.uits.iupui.edu', '200801051412.m05ECIaH010327@nakamura.uits.iupui.edu', '200801042044.m04Kiem3007881@nakamura.uits.iupui.edu', '200801041403.m04E3psW006926@nakamura.uits.iupui.edu', '200801032133.m03LX3gG005191@nakamura.uits.iupui.edu', '200801032205.m03M5Ea7005273@nakamura.uits.iupui.edu', '200801040947.m049lUxo006517@nakamura.uits.iupui.edu', 'gsilver@umich.edu', 'josrodri@iupui.edu', 'source@collab.sakaiproject.org', '200801042109.m04L92hb007923@nakamura.uits.iupui.edu', 'louis@media.berkeley.edu', '200801041633.m04GX6eG007292@nakamura.uits.iupui.edu', '200801032216.m03MGhDa005292@nakamura.uits.iupui.edu', 'hu2@iupui.edu', '200801041948.m04JmdwO007705@nakamura.uits.iupui.edu', '200801032122.m03LMFo4005148@nakamura.uits.iupui.edu', '200801040023.m040NpCc005473@nakamura.uits.iupui.edu', '200801042308.m04N8v6O008125@nakamura.uits.iupui.edu', '200801041106.m04B6lK3006677@nakamura.uits.iupui.edu', '200801041609.m04G9EuX007197@nakamura.uits.iupui.edu', '200801041537.m04Fb6Ci007092@nakamura.uits.iupui.edu', 'postmaster@collab.sakaiproject.org', '200801042001.m04K1cO0007738@nakamura.uits.iupui.edu', 'antranig@caret.cam.ac.uk'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "def print_email(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        email_patten = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        email = set(re.findall(email_patten, content))\n",
    "\n",
    "    return email\n",
    "# usage\n",
    "file_path = 'email_exchanges.txt'\n",
    "email_address = print_email(file_path)\n",
    "print(email_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the most common words in the English language. Call the name of your function find_most_common_words, it will take two parameters - a string or a file and a positive integer, indicating the number of words. Your function will return an array of tuples in descending order. Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('edu', 461), ('2008', 380), ('Jan', 352), ('sakaiproject', 340)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "def most_common_words(file_path, n):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        word_pattern = r'\\b\\w+\\b'\n",
    "        words = re.findall(word_pattern, content)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_count = word_counter.most_common(n)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "\n",
    "# usage\n",
    "file_path = 'email_exchanges.txt'\n",
    "print(most_common_words(file_path, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use the function, find_most_frequent_words to find: a) The ten most frequent words used in Obama's speech b) The ten most frequent words used in Michelle's speech c) The ten most frequent words used in Trump's speech d) The ten most frequent words used in Melina's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 120), ('and', 107), ('of', 81), ('to', 66), ('our', 58), ('we', 50), ('that', 49), ('a', 48), ('is', 36), ('us', 23)]\n"
     ]
    }
   ],
   "source": [
    "# a) The ten most frequent words used in Obama's speech\n",
    "\n",
    "most_frequent_words_obama_speech = most_common_words('obama_speech.txt', 10)\n",
    "print(most_frequent_words_obama_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 84), ('and', 80), ('the', 78), ('of', 46), ('that', 43), ('â', 42), ('a', 41), ('in', 36), ('I', 28), ('he', 28)]\n"
     ]
    }
   ],
   "source": [
    "# b) The ten most frequent words used in Michelle's speech\n",
    "most_frequent_words_Michelle_speech = most_common_words('Michelle_obama_speech.txt', 10)\n",
    "print(most_frequent_words_Michelle_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 61), ('and', 53), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('we', 26), ('is', 20), ('We', 17), ('America', 16)]\n"
     ]
    }
   ],
   "source": [
    "# c) The ten most frequent words used in Trump's speech\n",
    "most_frequent_words_Trump_speech = most_common_words('Donalt_speech.txt', 10)\n",
    "print(most_frequent_words_Trump_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 73), ('to', 54), ('the', 48), ('is', 29), ('I', 28), ('for', 27), ('of', 25), ('a', 22), ('that', 19), ('you', 18)]\n"
     ]
    }
   ],
   "source": [
    "# d) The ten most frequent words used in Melina's speech\n",
    "most_frequent_words_Melina_speech = most_common_words('Melina_trump_speech.txt', 10)\n",
    "print(most_frequent_words_Melina_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a python application that checks similarity between two texts. It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. For instance check the similarity between the transcripts of Michelle's and Melina's speech. You may need a couple of functions, function to clean the text(clean_text), function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). List of stop words are in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='stop_words.py' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the stop word file\n",
    "open('stop_words.py', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "import re\n",
    "from collections import Counter\n",
    "def clean_text(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)  #removed punctuation\n",
    "\n",
    "    return clean_text.lower().split()   #convert to lower case and removes white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "import stop_words \n",
    "def remove_stop_words(text, stop_words):\n",
    "    \n",
    "    words = text.split()\n",
    "    # filter the words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check word similarity\n",
    "def check_word_similarity(text1, text2):\n",
    "    \n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "\n",
    "    common_words = words1.intersection(words2)\n",
    "    total_words = words1.union(words2)\n",
    "\n",
    "    similarity = len(common_words)/len(total_words)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop woords\n",
    "def load_stop_words(filepath):\n",
    "    with open(filepath) as f:\n",
    "        return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    stop_words_path = 'stop_words.txt'\n",
    "    stop_words =  load_stop_words(stop_words_path)\n",
    "\n",
    "\n",
    "    text1_path = 'Michelle_obama_speech.txt'\n",
    "    text2_path = 'Melina_trump_speech.txt'\n",
    "\n",
    "    # load the files\n",
    "    with open(text1_path, 'r') as file:\n",
    "        text1 = file.read()\n",
    "\n",
    "    with open(text2_path, 'r') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    # cleean text\n",
    "    text1 = clean_text(text1_path)\n",
    "    text2 = clean_text(text2_path)\n",
    "\n",
    "    # remove stopwords\n",
    "    text1 = remove_stop_words(text1_path, stop_words)\n",
    "    text2 = remove_stop_words(text2_path, stop_words)\n",
    "\n",
    "    # check similarity\n",
    "    similarity = check_word_similarity(text1, text2)\n",
    "    return f\"similarity score: {similarity:.2f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity score: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity score: 0.24\n"
     ]
    }
   ],
   "source": [
    "# clean the text\n",
    "import re\n",
    "from collections import Counter\n",
    "def clean_text(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)  #removed punctuation\n",
    "    return ' '.join(clean_text.lower().split())   #convert to lower case and removes white space\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text, stop_words):\n",
    "    words = text.split()\n",
    "    # filter the words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# check word similarity\n",
    "def check_word_similarity(text1, text2):\n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "    common_words = words1.intersection(words2)\n",
    "    total_words = words1.union(words2)\n",
    "    similarity = len(common_words)/len(total_words)\n",
    "    return similarity\n",
    "\n",
    "# load stop woords\n",
    "def load_stop_words(filepath):\n",
    "    with open(filepath) as f:\n",
    "        return f.read().splitlines()\n",
    "    \n",
    "# main function\n",
    "def main():\n",
    "    stop_words_path = 'stop_words.txt'\n",
    "    stop_words =  load_stop_words(stop_words_path)\n",
    "\n",
    "\n",
    "    text1_path = 'Michelle_obama_speech.txt'\n",
    "    text2_path = 'Melina_trump_speech.txt'\n",
    "\n",
    "    # load the files\n",
    "    with open(text1_path, 'r') as file:\n",
    "        text1 = file.read()\n",
    "\n",
    "    with open(text2_path, 'r') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    # cleean text\n",
    "    text1 = clean_text(text1)\n",
    "    text2 = clean_text(text2)\n",
    "\n",
    "    # remove stopwords\n",
    "    text1 = remove_stop_words(text1, stop_words)\n",
    "    text2 = remove_stop_words(text2, stop_words)\n",
    "\n",
    "    # check similarity\n",
    "    similarity = check_word_similarity(text1, text2)\n",
    "    return f\"similarity score: {similarity:.2f}\"\n",
    "\n",
    "\n",
    "\n",
    "# usage:\n",
    "print(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Find the 10 most repeated words in the romeo_and_juliet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='romeo_and_juliet.txt' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('romeo_and_juliet.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 768), ('I', 655), ('to', 566), ('and', 562), ('of', 487), ('a', 462), ('in', 342), ('is', 338), ('you', 321), ('my', 310)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def repeated_words(file_path, n):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        word_pattern = r'\\b\\w+\\b'\n",
    "        words = re.findall(word_pattern, content)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "\n",
    "    return word_counter.most_common(n)\n",
    "\n",
    "file_path = 'romeo_and_juliet.txt'\n",
    "print(repeated_words(file_path, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the hacker news csv file and find out: a) Count the number of lines containing python or Python b) Count the number lines containing JavaScript, javascript or Javascript c) Count the number lines containing Java and not JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='hacker.csv' mode='w' encoding='cp1252'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('hacker.csv', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
